{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepstack import Member\n",
    "from deepstack import DirichletEnsemble\n",
    "from deepstack import StackEnsemble\n",
    "from keras.models import Sequential\n",
    "import random\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import cifar10\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "\n",
    "\n",
    "def load_sample_cifar_dataset(trainsample=5000, testsample=500):\n",
    "    \"\"\"\n",
    "    Loads a sample of cifar dataset. For training it creates a random sampling.\n",
    "    For validation and testing it creates a fixed sample.\n",
    "    The rationale is to train algorithms on different training sets but validate and test on the same dataset in order\n",
    "    to guarantee comparability.\n",
    "    Args:\n",
    "        trainsample: site of training set\n",
    "        testsample: size of validation / test set\n",
    "\n",
    "    Returns: x,y datasets for train, test and validation\n",
    "    \"\"\"\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    trainindex = random.sample(list(range(x_train.shape[0])), trainsample)\n",
    "    return x_train[trainindex, :, :, :], y_train[trainindex, :], x_test[0:testsample, :, :, :], y_test[0:testsample, :], x_test[testsample:testsample * 2, :, :, :], y_test[testsample:testsample * 2, :]\n",
    "\n",
    "\n",
    "def create_random_cnn(input_shape):\n",
    "    \"\"\"\n",
    "    Creates a CNN, based on random layer size.\n",
    "    Idea is to generate similar CNN models per function call\n",
    "    Args:\n",
    "        input_shape: the input_shape of the model\n",
    "\n",
    "    Returns: a keras CNN model\n",
    "    \"\"\"\n",
    "    weight_decay = 1e-4\n",
    "    num_classes = 10\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay),\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(random.randint(16, 64), (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(random.randint(0, 5) * 0.1))\n",
    "\n",
    "    model.add(Conv2D(random.randint(16, 64), (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(random.randint(16, 64), (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(random.randint(0, 5) * 0.1))\n",
    "\n",
    "    model.add(Conv2D(random.randint(64, 128), (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(random.randint(64, 128), (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(random.randint(0, 5) * 0.1))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_random_cifar_model(batch_size=32, epochs=100):\n",
    "    \"\"\"\n",
    "    Creates and fits a (random) CNN on the CIFAR10 dataset.\n",
    "    Args:\n",
    "        batch_size: the batch size for training the CNN model\n",
    "        epochs: epochs to train the model\n",
    "\n",
    "    Returns: fitted CNN model for the Cifar10 dataset, validation batches and test batches\n",
    "    \"\"\"\n",
    "    opt_rms = keras.optimizers.rmsprop(lr=0.001, decay=1e-6)\n",
    "    datagen = ImageDataGenerator(rotation_range=90,\n",
    "                                 width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = load_sample_cifar_dataset()\n",
    "    model = create_random_cnn(input_shape=x_train.shape[1:])\n",
    "    datagen.fit(x_train)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "    es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size, epochs=epochs,\n",
    "                        verbose=0, validation_data=(x_val, y_val), callbacks=[es_callback])\n",
    "\n",
    "    validation_batches = datagen.flow(x_val, y_val, batch_size=batch_size)\n",
    "    test_batches = datagen.flow(x_test, y_test, batch_size=batch_size)\n",
    "    return model, validation_batches, test_batches\n",
    "\n",
    "\n",
    "def cifar10_example(nmembers=4):\n",
    "    \"\"\"\n",
    "    Runs 2 DeepStack Ensemble Models for the Cifar Dataset\n",
    "    Args:\n",
    "        nmembers: amount of ensemble members to be generated\n",
    "\n",
    "    Returns: an instance of StackEnsemble and DirichletEnsemble for the Cifar10 dataset\n",
    "    \"\"\"\n",
    "    stack = StackEnsemble()\n",
    "    stack.model = RandomForestRegressor(verbose=1, n_estimators=300*nmembers, max_depth=nmembers*2, n_jobs=4)  # Meta-Learner\n",
    "\n",
    "    dirichletEnsemble = DirichletEnsemble(N=2000*nmembers)\n",
    "\n",
    "    for i in range(nmembers):\n",
    "        model, training_batch, validation_batch = get_random_cifar_model()  # Creates a Random CNN Keras Model for Cifar10 Dataset\n",
    "        # Rationale: The Validation and Testing dataset of a base-learner is the Training and Validation Dataset of a Meta-Learner\n",
    "        # Idea is to avoid validating the meta-learner on data that the base-learner has already seen on training\n",
    "        member = Member(name=\"model\"+str(i+1), keras_model=model, train_batches=training_batch, val_batches=validation_batch)  # Base-Learners\n",
    "        stack.add_member(member)  # Adds base-learner to Stack Ensemble\n",
    "        dirichletEnsemble.add_member(member)  # Adds base-learner to Dirichlet Ensemble\n",
    "\n",
    "    stack.fit()\n",
    "    dirichletEnsemble.fit()\n",
    "\n",
    "    return stack, dirichletEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 42ms/step\n",
      "16/16 [==============================] - 0s 29ms/step\n",
      "16/16 [==============================] - 1s 38ms/step\n",
      "16/16 [==============================] - 0s 29ms/step\n",
      "16/16 [==============================] - 1s 39ms/step\n",
      "16/16 [==============================] - 0s 29ms/step\n",
      "16/16 [==============================] - 1s 32ms/step\n",
      "16/16 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    2.8s finished\n"
     ]
    }
   ],
   "source": [
    "stack, dirichletEnsemble = cifar10_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 - AUC: 0.8044\n",
      "model2 - AUC: 0.8439\n",
      "model3 - AUC: 0.8218\n",
      "model4 - AUC: 0.8487\n",
      "StackEnsemble AUC: 0.8727073730966671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8727073730966671"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 - Weight: 0.1055 - AUC: 0.8044\n",
      "model2 - Weight: 0.2882 - AUC: 0.8439\n",
      "model3 - Weight: 0.2127 - AUC: 0.8218\n",
      "model4 - Weight: 0.3936 - AUC: 0.8487\n",
      "DirichletEnsemble AUC: 0.8821468197009587\n"
     ]
    }
   ],
   "source": [
    "dirichletEnsemble.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
